## Manual Operation Instructions
1. Go [here]({Outputs.saJobOutputsUrl02}) to update the outputs of the Stream Analytics job. In each, then click on the __{Outputs.pbiOutputName}__  outputs, there are 5 in total, and then click on __Renew Authorization__ to set up authorization for Stream Analytics Power BI Output. 
	- Once you have updated your credentials be sure to click *SAVE* before navigating to another output.
	- When all outputs have been updated, navigate [here]({Outputs.saJobOutputsUrl02}) and  click *START* to get the stream job started. Leave the selection on Job Start time.
2. In general, to start/stop your Stream Analytics Job, you may use the START / STOP commands [here]({Outputs.saJobOutputsUrl02}) 
3. You can go to [Power BI Dashboard](https://powerbi.microsoft.com/) and use a Real-time dataset to build reports and dashboards using your data!
	- On the [Power BI Dashboard](https://powerbi.microsoft.com/)  site locate the datasets produced by this pattern.
    - There are 5 Power BI Datasets generated by the Azure Stream Analytics job named waypoint0-waypoint4.
	- For each datasest
		- Click on the dataset name under __Datasets__
		- Under __Fields__ select the checkbox for ***device_id***
		- Under __Visualizations__ select ***Gauge***
		- Under the __File__ menu select ***Save As*** and name it with the name of the dataset (i.e. waypoint0-waypoint4)
		- Under __Reports__ click on the report created in the previous step
		- On the report, click on the Gauge chart type then click the pin shaped icon 
			- If this is the first dataset, select ***New dashboard***, otherwise choose ***Existing dashboard*** and choose the dashboard you are creating.
		- For each query provided in the next step
			- Enter the query, from below, in the **Ask a question about your data** control
			- Expand the __Visualizations__ blade and click on format (shaped like a paint roller)
			- Turn on __Title__
			- Enter the description provided with the query
			- Click __Pin Visual__ in the upper right hand corner
		- Queries (where N is the dataset you are working with)
			- How many device_id with label=1 in waypoint N in the last 30 seconds 
				- Description: 30 Second Success
			- How many device_id with label=-1 in waypoint N in the last 30 seconds 
				- Description: 30 Second Failure
			- device_id with label=-1 in waypoint 0 in the last 30 seconds
				- Description: Failed Product
			- device_id with label=1 in waypoint 0 in the last 30 seconds
				- Description: Successful Product
			
	
## Monitor the progress from
1. Web Jobs
	
	The production line data is generated by an Azure Web Job (of type Continuous). You can monitor this web job [here]({Outputs.webJobLogsUrl}).
		
2. Azure Event Hub
	    
    Azure Event Hub receives all of the events sent by the continuous web job. You can monitor the hub activity by logging into the Azure Portal, locating the service bus namespace {Outputs.namespaceName} and finding 
	the event hub {Outputs.ingestEventHubName}.
	
3. ML WebService
	
    Five machine learning models are deployed as Azure WebServices and are consumed by the Azure Stream Analytics job.
	
	* You can view your machine learning web services by going [here](https://studio.azureml.net) and in the machine learning workspace {Outputs.mlWorkspaceId}.
	
## Extending the solution
This solution receives a fairly significant amount of data from the generating application. That data is used to call an Azure Machine Learning experiment for a prediction, which is then saved to a Power BI dataset, but the raw data is disregarded.

In a real-world scenario, this raw incoming data would be typically be saved for a number of reasons.

The following steps will guide you in setting up a raw data dump to Azure Storage of that raw databy creating the services to stream raw events to Azure Storage using date and time partitions.  

***NOTE*** This extension has no effect on how the demo works and is included to give the reader some hands on experience to stream data to storage from an Azure Event Hub

### Extension steps
 - Log into the [Azure Management Portal](https://ms.portal.azure.com) 
 - In the left hand menu select *Resource groups*
 - Locate the resource group  you created for this project and click on it displaying the resources associated with the group in the resource group blade
 - Locate the Azure Service Bus ({Outputs.namespaceName}) and then under Event Hubs click on {Outputs.ingestEventHubName}
 - Click on *Consumer Groups* and then __+Add consumer...__
 - Enter the name __extensionGroup__, then click __Create__. 
 - Close the Event Hub and Service Bus blades to view the Resource Group
 - At the top of the Resource Group blade click __+Add__.
 - In the *Search Everything* search box enter ***Stream Analytics job***
 - Choose ***Stream Analytics job*** from the results then click *Create*
 - Enter ***manufactureExtension*** as the name and choose the subscription, resource group and location for this solution.
 - Click *Create* and then close the blades until the *Resource groups* blade is visible. You may need to use the __Refresh__ button at the top of the blade. 
 - Click on the ***manufactureExtension*** job.
 - Click *Inputs* and then the __+Add__ button
    - Input Alias: InputHub
	- Source Type: Data Stream
	- Source: Event Hub
	- Subscription: Use event hub from current subscription
	- Service bus namespace: {Outputs.namespaceName}
	- Event hub name: {Outputs.ingestEventHubName}
	- Event hub policy name: RootManageSharedAccessKey
	- Event hub consumer group: extensionGroup
	- Event serialization format: JSON
	- Encoding: UTF-8 
	- Click __Create__
 - Click *Ouputs* and then the __+Add__ button
    - Output Alias: OutputBlob
	- Sink: Blob storage
	- Subscription: Use event hub from current subscription
	- Storage account: {Outputs.storageAccountName}
	- Storage account key: Provided for you
	- Container: Create new container
	- Container*(Name): rawevents (all lower case)
	- Event hub consumer group: extensionGroup
	- Path pattern: {date}/{time}
	- Date format: YYYY-MM-DD
	- Time format: HH
	- Event serialization format: JSON
	- Encoding: UTF-8 
	- Format: Line separated
	- Click __Create__
- Click *Query*
    - Modify the query to: SELECT * INTO OutputBlob FROM InputHub 
    - Click *SAVE*
	- Close the Query blade
	- Click __Start__ on the Stream Analytics job blade
	  - Job output start time __Now__
	  - Click __Start__
